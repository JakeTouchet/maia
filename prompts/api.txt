Class System:
    A Python class containing the vision model and the specific neuron to interact with.
    
    Attributes
    ----------
    neuron_num : int
        The unit number of the neuron.
    layer : string
        The name of the layer where the neuron is located.
    model_name : string
        The name of the vision model.
    model : nn.Module
        The loaded PyTorch model.
    
    Methods
    -------
    call_neuron(image_list: List[torch.Tensor]) -> Tuple[List[int], List[str]]
        Returns the neuron activation for each image in the input image_list as well as the original image (encoded into a Base64 string).

    def call_neuron(self, image_list: List[torch.Tensor]) -> Tuple[List[float], List[str]]:
        The function returns the neuron's maximum activation value (in int format) for each of the images in the list as well as the original image (encoded into a Base64 string).
        
        Parameters
        ----------
        image_list : List[torch.Tensor]
            The input image
        
        Returns
        -------
        Tuple[List[int], List[str]]
            For each image in image_list returns the activation value of the neuron on that image, and the original image encoded into a Base64 string.
        
        
        Examples
        --------
        # test the activation value of the neuron for the prompt "a dog standing on the grass"
        >>> prompt = ["a dog standing on the grass"]
        >>> image = tools.text2image(prompt)
        >>> activations, images = system.call_neuron(image)
        >>> for activation, image in zip(activations, images):
        >>>     tools.display(image, f"Activation: {activation}")
        
        # test the activation value of the neuron for the prompt "a dog standing on the grass" 
        # and the neuron activation value for the same image but with a lion instead of a dog
        >>> prompt = ["a dog standing on the grass"]
        >>> edits = ["replace the dog with a lion"]
        >>> all_images, all_prompts = tools.edit_images(prompt, edits)
        >>> activation_list, image_list = system.call_neuron(all_images)
        >>> for activation, image in zip(activation_list, image_list):
        >>>     tools.display(image, f"Activation: {activation}")


Class Tools:
    A Python class containing tools to interact with the units implemented in the system class, 
    in order to run experiments on it.
    
    Attributes
    ----------
    text2image_model_name : str
        The name of the text-to-image model.
    text2image_model : any
        The loaded text-to-image model.
    images_per_prompt : int
        Number of images to generate per prompt.
    path2save : str
        Path for saving output images.
    threshold : any
        Activation threshold for neuron analysis.
    device : torch.device
        The device (CPU/GPU) used for computations.
    experiment_log: str
        A log of all the experiments, including the code and the output from the neuron
        analysis.
    exemplars : Dict
        A dictionary containing the exemplar images for each unit.
    exemplars_activations : Dict
        A dictionary containing the activations for each exemplar image.
    exemplars_thresholds : Dict
        A dictionary containing the threshold values for each unit.
    results_list : List
        A list of the results from the neuron analysis.
    
    Methods
    -------
    dataset_exemplars()->Tuple[List[float], List[str]]
        This experiment provides good coverage of the behavior observed on a
        very large dataset of images and therefore represents the typical
        behavior of the neuron on real images. This function characterizes the
        prototypical behavior of the neuron by computing its activation on all
        images in the ImageNet dataset and returning the 15 highest activation
        values and the images that produced them.
    edit_images(self, images: Union[List[Image.Image], List[str]], prompts: List[str]) -> List[Image.Image]
        This function enables loclized testing of specific hypotheses about how
        variations on the content of a single image affect neuron activations.
        Gets a list of input prompt and a list of corresponding editing
        instructions, then generate images according to the input prompts and
        edits each image based on the instructions given in the prompt using a
        text-based image editing model. This function is very useful for testing
        the causality of the neuron in a controlled way, for example by testing
        how the neuron activation is affected by changing one aspect of the
        image. IMPORTANT: Do not use negative terminology such as "remove ...",
        try to use terminology like "replace ... with ..." or "change the color
        of ... to ...".
    text2image(prompt_list: str) -> List[str]
        Gets a list of text prompts as an input and generates an image for each
        prompt using a text to image model. The function returns a
        list of images in Base64 encoded string forma.
    summarize_images(self, image_list: List[str]) -> str:    
        This function is useful to summarize the mutual visual concept that
        appears in a set of images. It gets a list of images at input and
        describes what is common to all of them.
    describe_images(synthetic_image_list: List[str], synthetic_image_title:List[str]) -> str
        Provides impartial descriptions of images. Do not use this function on
        dataset exemplars. Gets a list of images and generates a textual
        description of the semantic content of each of them.
        The function is blind to the current hypotheses list and
        therefore provides an unbiased description of the visual content.
    def display(self, *args: Union[str, Image.Image, Iterable]):
        This function is your way of displaying experiment data. You must call
        this on results/variables that you wish to view in order to view them.     

    def text2image(self, prompt_list: List[str]) -> List[torch.Tensor]:
        Takes a list of text prompts and generates images_per_prompt images for each using a
        text to image model. The function returns a list of a list of images_per_prompt images 
        for each prompt.
        
        Parameters
        ----------
        prompt_list : List[str]
            A list of text prompts for image generation.
        
        Returns
        -------
        List[List[str]]
            A list of a list of images_per_prompt images in Base64 encoded string format for 
            each input prompts. 
        
        Examples
        --------
        >>> prompt_list = ["a dog standing on the grass", 
        >>>                "a dog sitting on a couch",
        >>>                "a dog running through a field"]
        >>> images = tools.text2image(prompt_list)
        >>> activation_list, image_list = system.call_neuron(images)
        >>> for activation, image in zip(activation_list, image_list):
        >>>     tools.display(image, f"Activation: {activation}")

    def edit_images(self, images: Union[List[PIL.Image.Image], List[str]], prompts: List[str]):
        Generate images from a list of prompts, then edits each image with the
        corresponding editing prompt. Important note: Do not use negative
        terminology such as "remove ...", try to use terminology like "replace
        ... with ..." or "change the color of ... to" The function returns a
        list of images and list of the relevant prompts.
        
        Parameters
        ----------
        images : images: Union[List[Image.Image], List[str]]
            A list of images to edit
        editing_prompts : List[str]
            A list of instructions for how to edit the images in image_list.
            Should be the same length as image_list.
        
        Returns
        -------
        List[Image.Image]
            The list of edited images
        
        Examples
        --------
        # test the units on the prompt "a dog standing on the grass" and
        # on the same image but with a cat instead of a dog
        >>> images = tools.text2image(prompts)
        >>> edits = ["replace the dog with a lion"]
        >>> edited_images = tools.edit_images(images, edits) 
        >>> activation_list, image_list = system.call_neuron(edited_images)
        >>> for activation, image in zip(activation_list, image_list):
        >>>     tools.display(image, f"Activation: {activation}")
        
        # test the activation value of unit 1 for the prompt "a dog standing on the grass"
        # for the same image but with a different action instead of "standing":
        >>> prompts = ["a dog standing on the grass"]*3
        >>> images = tools.text2image(prompts)
        >>> edits = ["make the dog sit","make the dog run","make the dog eat"]
        >>> edited_images = tools.edit_images(images, edits) 
        >>> activation_list, image_list = system.call_neuron(edited_images)
        >>> for activation, image in zip(activation_list, image_list):
        >>>     tools.display(image, f"Activation: {activation}")

    def dataset_exemplars(self) -> Tuple[List[float], List[str]]:
        This method finds images from the ImageNet dataset that produce the highest activation values for a specific neuron.
        It returns both the activation values and the corresponding exemplar images that were used to generate these activations.
        This experiment is performed on real images and will provide a good approximation of the neuron behavior.
        
        Returns
        -------
        List[float]
            The activations for each exemplar
        List[str]
            The masked images for each exemplars
        
        Example
        -------
        >>> activations, images = tools.dataset_exemplars()
        >>> for activation, image in zip(activations, images):
        >>>     tools.display(image, f"Activation: {activation}")

    def display(self, *args: Union[str, PIL.Image.Image, Iterable]):
        Displays a series of images and/or text in the chat, similar to a Jupyter notebook.
        
        Parameters
        ----------
        *args : Union[str, Image.Image, Iterable]
            The content to be displayed in the chat. Can be multiple strings or Image objects.
        
        Notes
        -------
        Displays directly to chat interface.
        
        Example
        -------
        # Display a single image
        >>> prompt = ["a dog standing on the grass"]
        >>> images = tools.text2image(prompt)
        >>> activation_list, image_list = system.call_neuron(images)
        >>> for activation, image in zip(activation_list, image_list):
        >>>     tools.display(image, f"Activation: {activation}")
        
        # Display a list of images
        >>> prompt_list = ["a dog standing on the grass", 
        >>>                "a dog sitting on a couch",
        >>>                "a dog running through a field"]
        >>> images = tools.text2image(prompt_list)
        >>> activation_list, image_list = system.call_neuron(images)
        >>> for activation, image in zip(activation_list, image_list):
        >>>     tools.display(image, f"Activation: {activation}")

    def describe_images(self, image_list: List[str], image_title: List[str]) -> str:
        Generates textual descriptions for a list of images, focusing
        specifically on highlighted regions. The final descriptions are
        concatenated and returned as a single string, with each description
        associated with the corresponding image title.
        
        Parameters
        ----------
        image_list : List[str]
            A list of images in Base64 encoded string format.
        image_title : List[str]
            A list of titles for each image in the image_list.
        
        Returns
        -------
        str
            A concatenated string of descriptions for each image, where each description 
            is associated with the image's title and focuses on the highlighted regions 
            in the image.
        
        Example
        -------
        >>> prompt_list = ["a dog standing on the grass", 
        >>>                "a dog sitting on a couch",
        >>>                "a dog running through a field"]
        >>> images = tools.text2image(prompt_list)
        >>> activation_list, image_list = system.call_neuron(images)
        >>> descriptions = tools.describe_images(image_list, prompt_list)
        >>> tools.display(descriptions)

    def summarize_images(self, image_list: List[str]) -> str:
        Gets a list of images and describes what is common to all of them.
        
        Parameters
        ----------
        image_list : list
            A list of images in Base64 encoded string format.
        
        Returns
        -------
        str
            A string with a descriptions of what is common to all the images.
        
        Example
        -------
        >>> # Summarize a neuron's dataset exemplars
        >>> exemplars = [exemplar for _, exemplar in tools.dataset_exemplars()] # Get exemplars
        >>> summarization = tools.summarize_images(exemplars)
        >>> tools.display(summarization)


