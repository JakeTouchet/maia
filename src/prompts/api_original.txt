class System:
    """
    A Python class for interfacing with specified units within vision models.
    
    Attributes
    ----------
    units : List[Unit]
        A list of units, each containing the model name, layer name, and neuron number.
    unit : Unit
        The current unit being analyzed.
    model_wrapper : ModelInfoWrapper
        The model wrapper for the current unit.
    model_dict : Dict[str, ModelInfoWrapper]
        A dictionary of model names and their corresponding ModelInfoWrapper objects.
    device : torch.device
        The device (CPU/GPU) used for computations.
    threshold : int
        The current activation threshold for neuron analysis.
    thresholds : Dict
        A dictionary containing the threshold values for each unit.
    
    Methods
    -------
    TESING:
    call_units(self, image_list: List[torch.Tensor], unit_ids:List[int])->List[List[Tuple[float, str]]]]]
        Primary function for testing how units respond to images via its
        activation values and areas of interest.
    """

    def __init__(self, unit_dict: Dict[str, Dict[str, List[int]]], thresholds: Dict, device: Union[int, str]):
        """
        Initializes a system for interfacing with a set of specified units.
        Parameters
        -------
        unit_dict : dict
            {
            model_name: {
                    layer_name: neuron_list
                    }
            }
        thesholds : dict
            Contains the threshold values for each unit
        device : str
            The computational device ('cpu' or 'cuda').
        """

    def call_units(self, image_list: List[torch.Tensor], unit_ids:List[int])->List[List[Tuple[float, str]]]:
        """
        For each specified unit, returns the unit’s maximum activation value
        (in int format) for each image. Also returns masked images that
        highlight the regions of the image where the activations are highest
        (encoded into a Base64 string).
    
        Note
        ----
        When performing an experiment on multiple units, pass them togethers as a list, 
        instead of one-by-one
        
        Parameters
        ----------
        image_list : List[torch.Tensor]
            The input image
        unit_ids : List[int]
            The unit ids to retrieve activations for.
        
        Returns
        -------
        List[List[Tuple[float, str]]]
            For each unit, stores the maximum activations and masked images as a
            list of tuples.       

        Examples
        --------
        >>> # Test the activation value of a single unit for a prompt
        >>> prompt = ["a man eating a gargantuan sandwich"]
        >>> images = tools.text2image(prompt)
        >>> activation, masked_image = system.call_units(images, [0])[0][0]
        >>> tools.display(activation, masked_image)
        >>>
        >>> # Test the activation value of multiple units for the prompt "a dog standing on the grass"
        >>> prompt = ["a dog standing on the grass"]
        >>> images = tools.text2image(prompt)
        >>> unit_ids = [0, 1]  # Example unit IDs to test
        >>> unit_data = system.call_units(images, unit_ids)
        >>> for unit_datum in unit_data:
        >>>     activation, masked_image = unit_datum[0]
        >>>     tools.display(activation, masked_image)
        >>>
        >>> # Test the activation value of multiple units for multiple prompts
        >>> prompt_list = ["a fox and a rabbit watch a movie under a starry night sky",
        >>>                "a fox and a bear watch a movie under a starry night sky",
        >>>                "a fox and a rabbit watch a movie at sunrise"]
        >>> images = tools.text2image(prompt_list)
        >>> unit_ids = [0, 1]  # Example unit IDs to test
        >>> unit_data = system.call_units(images, unit_ids)
        >>> for i, image in enumerate(images):
        >>>     tools.display(f"Image {i}:", image)
        >>>     for j, unit_datum in enumerate(unit_data):
        >>>         tools.display(f"unit {j}")
        >>>         activation, masked_image = unit_datum[i]
        >>>         tools.display(activation, masked_image)
        """

class Tools:
    """
    A Python class containing tools to interact with the units implemented in the system class, 
    in order to run experiments on it.

    Attributes
    ----------
    text2image_model_name : str
        The name of the text-to-image model.
    text2image_model : any
        The loaded text-to-image model.
    images_per_prompt : int
        Number of images to generate per prompt.
    path2save : str
        Path for saving output images.
    threshold : any
        Activation threshold for neuron analysis.
    device : torch.device
        The device (CPU/GPU) used for computations.
    experiment_log: str
        A log of all the experiments, including the code and the output from the neuron
        analysis.
    exemplars : Dict
        A dictionary containing the exemplar images for each unit.
    exemplars_activations : Dict
        A dictionary containing the activations for each exemplar image.
    exemplars_thresholds : Dict
        A dictionary containing the threshold values for each unit.
    results_list : List
        A list of the results from the neuron analysis.

    Methods
    -------
    EXPLORATION:
    dataset_exemplars(self, unit_ids: List[int], system: System)->List[List[Tuple[float, str]]]
        This experiment provides good coverage of the behavior observed on a
        very large dataset of images and therefore represents the typical
        behavior of the neuron on real images. This function characterizes the
        prototypical behavior of the neuron by computing its activation on all
        images in the ImageNet dataset and returning the 15 highest activation
        values and the images that produced them. The images are masked to
        highlight the specific regions that produce the maximal activation. The
        images are overlaid with a semi-opaque mask, such that the maximally
        activating regions remain unmasked.
    visdiff(self, system: System, mode="OBJECTS", k: int = 5, units: List[int] = [0, 1]) -> str:
        This function suggests visual differences between the exemplars of different units, based off
        a certain visual category. It's useful for teasing out subtle differences between
        two units, especially when you suspect they're detecting different kinds of features. Try to
        get a good concept of what each unit does before running this experiment, because what it returns
        is a list of differences, not a description of each unit.
    summarize_images(self, image_list: List[str]) -> str:    
        This function is useful to summarize the mutual visual concept that
        appears in a set of images. It gets a list of images at input and
        describes what is common to all of them, focusing specifically on
        unmasked regions.
    describe_images(synthetic_image_list: List[str], synthetic_image_title:List[str]) -> str
        Provides impartial descriptions of images. Do not use this function on
        dataset exemplars. Gets a list of images and generates a textual
        description of the semantic content of the unmasked regions within each
        of them. The function is blind to the current hypotheses list and
        therefore provides an unbiased description of the visual content.
        
    TESTING:
    text2image(prompt_list: str) -> Tuple[torch.Tensor]
        Gets a list of text prompts as an input and generates an image for each
        prompt using a text to image model. The function returns a
        list of images.
    edit_images(prompt_list_org_image : List[str], editing_prompts : List[str]) -> Tuple[List[Image.Image], List[str]]
        This function enables loclized testing of specific hypotheses about how
        variations on the content of a single image affect neuron activations.
        Gets a list of input prompt and a list of corresponding editing
        instructions, then generate images according to the input prompts and
        edits each image based on the instructions given in the prompt using a
        text-based image editing model. This function is very useful for testing
        the causality of the neuron in a controlled way, for example by testing
        how the neuron activation is affected by changing one aspect of the
        image. IMPORTANT: Do not use negative terminology such as "remove ...",
        try to use terminology like "replace ... with ..." or "change the color
        of ... to ...".
    display(self, *args: Union[str, Image.Image]):
        This function is your way of displaying experiment data. You must call
        this on results/variables that you wish to view.     

    def __init__(self, path2save: str, device: str, DatasetExemplars: DatasetExemplars = None, images_per_prompt=10, text2image_model_name='sd'):
        """
        Initializes the Tools object.

        Parameters
        ----------
        path2save : str
            Path for saving output images.
        device : str
            The computational device ('cpu' or 'cuda').
        DatasetExemplars : object
            an object from the class DatasetExemplars
        images_per_prompt : int
            Number of images to generate per prompt.
        text2image_model_name : str
            The name of the text-to-image model.
        """

    def dataset_exemplars(self, unit_ids: List[int], system: System)->List[List[Tuple[float, str]]];
        """
        Retrieves the activations and exemplar images for the specified units.

        Parameters
        ----------
        unit_ids : Union[List[int], int]
            Unit ids to retrieve exemplars for.
        system : System
            The system object containing the units to retrieve exemplars for.

        Returns
        -------
        List[List[Tuple[float, str]]]
            For each unit, stores the maximum activations and masked images as a tuple.

        Example
        -------
        >>> # Display the exemplars and activations for a list of units
        >>> unit_ids = [0, 1]
        >>> exemplar_data = tools.dataset_exemplars(unit_ids, system)
        >>> for i, exemplar_datum in enumerate(exemplar_data):
        >>>     tools.display(f"unit {unit_ids[i]}: ")
        >>>     for activation, masked_image in exemplar_datum:
        >>>         tools.display(masked_image, activation)
        """
        
    def edit_images(self, image_prompts : List[str], editing_prompts : List[str]):
        """
        Generate images from a list of prompts, then edits each image with the
        corresponding editing prompt. The function returns a
        list of images and list of the relevant prompts.

        Parameters
        ----------
        image_prompts : List[str]
            A list of input ptompts to generate images according to, these
            images are to be edited by the prompts in editing_prompts.
        editing_prompts : List[str]
            A list of instructions for how to edit the images in image_list.
            Should be the same length as image_list.

        Returns
        -------
        List[Image.Image], List[str]
            A list of images and a list of all the prompts that
            were used in the experiment, in the same order as the images

        Examples
        --------
        >>> # test the units on the prompt "a dog standing on the grass" and
        >>> # on the same image but with a cat instead of a dog
        >>> prompt = ["a dog standing on the grass"]
        >>> edits = ["replace the dog with a cat"]
        >>> all_images, all_prompts = tools.edit_images(prompt, edits)
        >>> unit_ids = [0, 1]
        >>> unit_data = system.call_units(all_images, unit_ids)
        >>>
        >>> for i in range(len(all_images)):
        >>>     tools.display(all_images[i], all_prompts[i])
        >>>     for j in range(len(unit_data)):
        >>>         activations, masked_images = unit_data[j]
        >>>         tools.display(f"unit {j} masked image: ", masked_images[i])
        >>>         tools.display(f"unit {j} activation: ", activations[i])
        >>> 
        >>> # test the activation value of unit 1 for the prompt "a dog standing on the grass"
        >>> # for the same image but with a different action instead of "standing":
        >>> prompts = ["a dog standing on the grass"]*3
        >>> edits = ["make the dog sit","make the dog run","make the dog eat"]
        >>> all_images, all_prompts = tools.edit_images(prompts, edits)
        >>> unit_data = system.call_units(all_images, [1])
        >>> activations, masked_images = unit_data[0]
        >>> for i in range(len(all_images)):
        >>>     tools.display(all_images[i], all_prompts[i])
        >>>     tools.display(f"unit 1 masked image: ", masked_images[i])
        >>>     tools.display(f"unit 1 activation: ", activations[i])
        """

    def text2image(self, prompt_list: List[str]) -> List[Image.Image]:
        """
        Takes a list of text prompts and generates an image for each using a
        text to image model. The function returns a list of images.

        Parameters
        ----------
        prompt_list : List[str]
            A list of text prompts for image generation.

        Returns
        -------
        List[Image.Image]
            A list of images, corresponding to each of the input prompts. 


        Examples
        --------
        >>> # Generate images from a list of prompts
        >>>     prompt_list = [“a toothbrush on mars”, 
        >>>                     “a toothbrush on venus”,
        >>>                     “a toothbrush on pluto”]
        >>>     images = tools.text2image(prompt_list)
        >>>     tools.display(*images)
        """

    def display(self, *args: Union[str, Image.Image]):
        """
        Displays a series of images and/or text in the chat, similar to a Jupyter notebook.
        
        Parameters
        ----------
        *args : Union[str, Image.Image]
            The content to be displayed in the chat. Can be multiple strings or Image objects.
        """

        Notes
        -------
        Displays directly to chat interface.

        Example
        -------
        >>> # Display a single image
        >>> prompt = ["a dog standing on the grass"]
        >>> images = tools.text2image(prompt)
        >>> tools.display(*images)
        >>>
        >>> # Display a list of images
        >>> prompt_list = ["A green creature",
        >>>                 "A red creature",
        >>>                 "A blue creature"]
        >>> images = tools.text2image(prompt_list)
        >>> tools.display(*images)
        """

    def visdiff(self, system: System, mode="OBJECTS", k: int = 5, units: List[int] = [0, 1]) -> str:
        """
        Detects the top k visual differences between one set of exemplars and the other. The mode determines which
        visual features to look for.

        Parameters
        ----------
        system : System
            The system object containing the units to compare.
        mode : str
            The mode of comparison, can be "OBJECTS", "COLORS", "TEXTURES", "SHAPES", "PATTERNS", or "ANY".
        k : int
            The number of differences to return.
        units : List[int]
            The subject unit. Can be [0], [1], or [0, 1] for bidirectional comparison.
        
        Returns
        -------
        str
            A string containing the top k visual differences between the two units.
        
        Notes
        -------
        This tool can only highlight things more present in one unit than the
        other. It is not a description of each unit. Just because a unit has
        more trees, grass, and bushes than the other, doesn't mean that unit
        cares only about trees, grass, and bushes. It might attend more to the
        color green or natural objects, or it could be a coincidence.

        After receiving the list of differences, consider what's common among
        them and whether rerunning the tool with a different mode might reveal a
        more accurate description.

        Also, if there are a wide variety of seemingly unrelated responses, like:

        Unit 0 has more 
        ['"wooden objects"', '"leaves"', 'food items', 'dolls', 'clothing']
        than unit 1.

        Then it's likely that the mode you chose did not reveal the true difference.
        Try rerunning with a different mode until there is a clear pattern.

        Example
        -------
        >>> # You suspect the two units are detecting different objects
        >>> # Run VisDiff to confirm
        >>> output = tools.visdiff(system, mode="OBJECTS", units=[0, 1])
        >>> tools.display(output)
        >>>
        >>> # You think unit 1 is detecting different colors and patterns
        >>> # Run VisDiff to confirm
        >>> colors_output = tools.visdiff(system, mode="COLORS", units=[1])
        >>> patterns_output = tools.visdiff(system, mode="PATTERNS", units=[1])
        >>> tools.display(colors_output, patterns_output)
        >>>
        >>> # You think unit 0 cares about zebra stripes, while unit 1 cares about the whole zebra
        >>> # Run VisDiff to confirm
        >>> output0 = tools.visdiff(system, mode="PATTERNS", units=[0])
        >>> output1 = tools.visdiff(system, mode="OBJECTS", units=[1])
        >>> tools.display(output0, output1)

    def summarize_images(self, image_list: List[str]) -> str:
        """
        Gets a list of images and describes what is common to all of them, focusing 
        specifically on unmasked regions.


        Parameters
        ----------
        image_list : list
            A list of images in Base64 encoded string format.
        
        Returns
        -------
        str
            A string with a descriptions of what is common to all the images.

        Example
        -------
        >>> # Summarize a unit's dataset exemplars
        >>> exemplars = [exemplar for _, exemplar in tools.dataset_exemplars([0], system)[0]] # Get exemplars for unit 0
        >>> summarization = tools.summarize_images(exemplars)
        >>> tools.display(summarization)
        >>>
        >>> # Summarize what's common amongst two sets of exemplars
        >>> exemplars_data = tools.dataset_exemplars([0,1], system)
        >>> all_exemplars = []
        >>> for exemplar_datum in exemplars_data:
        >>>     for _, exemplar in exemplar_datum:
        >>>         all_exemplars.append(exemplar)
        >>> summarization = tools.summarize_images(all_exemplars)
        >>> tools.display(summarization)
        """

    def describe_images(self, image_list: List[str], image_title:List[str]) -> str:
        """
        Generates textual descriptions for a list of images, focusing
        specifically on highlighted regions. The final descriptions are
        concatenated and returned as a single string, with each description
        associated with the corresponding image title.

        Parameters
        ----------
        image_list : List[str]
            A list of images in Base64 encoded string format.
        image_title : List[str]
            A list of titles for each image in the image_list.

        Returns
        -------
        str
            A concatenated string of descriptions for each image, where each description 
            is associated with the image's title and focuses on the highlighted regions 
            in the image.

        Example
        -------
        >>> prompt_list = [“a green dog”, 
                            “a green frog”,
                            “a green log”]
        >>> images = tools.text2image(prompt_list)
        >>> activation_list, masked_images = system.units(images, [0])[0]
        >>> descriptions = tools.describe_images(masked_images, prompt_list)
        >>> tools.display(descriptions)
        """